* Kazoo AMQP

This library manages KAZOO's interaction with AMQP brokers ([[https://www.rabbitmq.com/][RabbitMQ]] is the broker of choice).

** Erlang library architecture

The first thing when setting up AMQP interactions is to start at least one connection. This is just a simple TCP connection, pulled from the =config.ini= file.

#+begin_src plantuml :file process_tree.png :exports results
skinparam monochrome true
object kz_amqp_connections {
  type = "gen_server"
}
object kz_amqp_connection_sup {
  type = "supervisor"
}
object kz_amqp_assignments {
  type = "gen_server"
}
object kz_amqp_bootstrap {
  type = "gen_server"
}

#+end_src

#+RESULTS:
[[file:process_tree.png]]

*** =kz_amqp_connections= worker

This worker starts a =gen_server= to manage an ETS table of the AMQP connections started. New connections will be registered with this worker who will track the connection processes (start via the [[https://www.rabbitmq.com/erlang-client-user-guide.html][=amqp_client=]] library's =amqp_connection= module).

Since the AMQP connections may or may not be ready, there are facilities for calling processes to wait until the connection is ready.

*** =kz_amqp_connection_sup= supervisor

Adds and removes =kz_amqp_connection= processes.

*** =kz_amqp_assignments= worker

This worker starts a =gen_server= to manage an ETS table of the AMQP channels assigned to KAZOO process PIDs.

There are two types of =channels= KAZOO manages: =float= and =sticky=.

**** Float

FLoating channels are assigned to the current primary broker. When the broker goes down, the channel is moved to the secondary (tertiary, which ever is next in line) broker. When the primary broker recovers, the channel will be moved back to the primary broker.

**** Sticky

These channels are assigned to a specific broker and stay there.

*** =kz_amqp_bootstrap= worker

This worker starts a =gen_server= that loads the AMQP config (per-zone if configured) and instructs =kz_amqp_connections= to add the broker(s). This process will then block until the AMQP broker connections are established in =init/1= (effectively blocking the startup of the VM).

*** = kz_amqp_connection= worker

This worker starts a =gen_server= to manage an =amqp_connection= process from the =amqp_client= library.

The first activity after the connection is established is to start =prechannels= which are channels created ahead of time. Since creating the channel is relatively expensive, KAZOO maintains a buffer of prechannels to aid in speeding up calling code's ability to interact with the broker. As prechannels are assigned to consumers, new prechannels are created to replace the now-assigned channels.

The worker then starts a channel and assigns it as the default consumer for the connection.

** Testing
Test gen_listener + listener_federator timeout during shutdown

* Investigations
** TODO Detect / handle flow control
https://www.rabbitmq.com/flow-control.html

Flow control can be applied on connections, channels, or queues, and affects publishing of messages
** TODO Dedicated connection for publishing-only
Contention between fast publishers and slow consumers on the same connection can invoke flow control (artificially slowing down the connection speed). Related to QOS / prefetch settings.

** TODO 1. Fix the services cache bindings
** TODO 2. Create a means to disable all MODb updates without impacting applications / services
** TODO 3. Deploy a regex against all bindings keys to replace only and trailing wildcards with '#'
** TODO 4. Move to application pools to avoid queue churn
1. Added benefit that it also limits resources consumption on rabbitmq

4.3 backport

Currently each gen_listener uses a channel and, once finished, closes the channel. We papered over this a bit with maintaining prechannels (since creating a channel is a network op). Generally speaking, one kazoo process doing AMQP consuming gets a dedicated channel. This is not required.

https://www.rabbitmq.com/channels.html

We could, instead, maintain pools of workers per-app that are checked in/out and tuned as traffic increases. This would eliminate the majority of channel churn (painful on the broker too) and potentially improve performance. The downside is the resting state of the system is a higher baseline of memory consumption. An auto-scaling pool could be of interest for high volume apps like callflows while a more static pool could be useful for apps with minimal channel usage like teletype.

** DONE 5. For non-named queues remove or significantly increase the flow control limit
CLOSED: [2019-04-02 Tue 20:56]
https://www.rabbitmq.com/confirms.html
"Finding a suitable prefetch value is a matter of trial and error and will vary from workload to workload. Values in the 100 through 300 range usually offer optimal throughput and do not run significant risk of overwhelming consumers."

50 was picked as a conservative default but config.ini can override this default as necessary.

** TODO 6. Audit all gen_listeners for use of 'self' bindings when not necessary
** TODO 7. Move direct message bindings to built in (and implicit) AMQP direct exchange
** TODO 8. Determine and reduce the need for 171 bindings on zswitch for database creation / removal
** TODO 9. Create an 'AMQP router' in kazoo core that will bind for all configuration events and distribute locally in the Erlang VM
